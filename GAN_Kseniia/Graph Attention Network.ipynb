{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Graph Attention Network](https://arxiv.org/abs/1710.10903)\n",
    "\n",
    "The way in which *GCN* aggregates is structure-dependent, which may hurt its generalizability.<br>\n",
    "*GAT* proposes an alternative way to combine local graph structure and features by weighting neighbor features with feature dependent and structure free normalization, in the style of *attention*. <br>\n",
    "This is different from what is done in [*GraphSAGE*](https://www-cs-faculty.stanford.edu/people/jure/pubs/graphsage-nips17.pdf), i.e. average over all neighbors.\n",
    "\n",
    "## Graph Convolution operation\n",
    "\n",
    "The key difference between GAT and GCN is how the information from the one-hop neighborhood is aggregated.<br>\n",
    "For GCN, a graph convolution operation produces the normalized sum of the node features of neighbors:\n",
    "\n",
    "$$ h^{(l+1)}_i = \\sigma(\\ \\sum_{j \\in \\mathcal{N_{(i)}} } W^{(l)}h^{(l)}_j\\ )$$\n",
    "\n",
    "where:\n",
    "*  $\\mathcal{N_{(i)}}$ is the set of its one-hop neighbors (to include $v_i$ add a self-loop to each node)\n",
    "\n",
    "*  $c_{ij}$ is a normalization constant based on graph structure, \n",
    "\n",
    "*  $\\sigma$ is an activation function (GCN uses ReLU)\n",
    "\n",
    "*  W(l) is a shared weight matrix for node-wise feature transformation\n",
    "\n",
    "## Introducing Attention\n",
    "\n",
    "GAT introduces the attention mechanism as a substitute for the statically normalized convolution operation. The layer maps a set of node features $h \\in \\mathbb{R}^F$ to a new representation $h^{l+1} \\in \\mathbb{R}^{F^{'}}$. Below are the equations to compute the node embedding $h^{(l+1)}_i$ of layer $l+1$ from the embeddings of layer $l$\n",
    ":\n",
    "<img src=https://s3.us-east-2.amazonaws.com/dgl.ai/tutorial/gat/gat.png width=400>\n",
    "\n",
    "\n",
    "$$\\begin{equation}\n",
    "z_i^{(l)}=W^{(l)}h_i^{(l)}\n",
    "\\tag{1}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "e_{ij}^{(l)}=\\text{LeakyReLU}(\\vec a^{(l)^T}(z_i^{(l)}||z_j^{(l)}))\n",
    "\\tag{2}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\alpha_{ij}^{(l)}=\\frac{\\exp(e_{ij}^{(l)})}{\\sum_{k\\in \\mathcal{N}(i)}^{}\\exp(e_{ik}^{(l)})}\n",
    "\\tag{3}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "h_i^{(l+1)}=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\alpha^{(l)}_{ij} z^{(l)}_j }\\right)\n",
    "\\tag{4}\\end{equation}$$\n",
    "\n",
    "*  (1)  As an initial step a shared learnable linera transformation parametrized by the weight matrix $W^{(l)}$ is applies to every node\n",
    "<br>\n",
    "\n",
    "*  (2)  Computes a pair-wise *unnormalized* attention score $e_{ij}$ between two neighbors, i.e. importance of node $j$'s features to node $i$. Here, it first concatenates the $z$ embeddings of the two nodes, where $||$ denotes concatenation, and then takes the dot product of it with a learnable weight vector $\\vec a^{(l)}$, which represents the attention mechanism.\n",
    "<br>\n",
    "\n",
    "*  (3) To make coefficients easily comparable we normalize the score using a softmax function\n",
    "<br>\n",
    "\n",
    "*  (4) The embeddings from neighbors are aggregated together, scaled by the attention scores.\n",
    "\n",
    "*N.B.* the layer applies a **masked** self-attention to the input since the aggregation is carried out only over the neighbors "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT with dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # equation (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation (1)\n",
    "$$z^{(l)}_i=W^{(l)}h^{(l)}_i$$\n",
    "<br>\n",
    "Is simply a linear transformation and can be easily implemented in Pytorch using **torch.nn.Linear**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation (2)\n",
    "\n",
    "$$e_{ij}^{(l)}=\\text{LeakyReLU}(\\vec a^{(l)^T}(z_i^{(l)}||z_j^{(l)}))$$\n",
    "<br>\n",
    "\n",
    "The unnormalized attention score $e_{ij}$ is calculated using the embeddings of adjacent nodes $i$ and $j$. This suggests that the attention scores can be viewed as edge data which can be calculated by the **apply_edges** API. The argument to the apply_edges is an Edge UDF, which is defined as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_attention(self, edges):\n",
    "    # edge UDF for equation (2)\n",
    "    z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "    a = self.attn_fc(z2)\n",
    "    return {'e' : F.leaky_relu(a)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the dot product with the learnable weight vector $\\vec a^{(l)}$ is implemented again using pytorchâ€™s linear transformation *attn_fc*. Note that apply_edges will batch all the edge data in one tensor, so the cat, attn_fc here are applied on all the edges in parallel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equations (3) & (4)\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\alpha_{ij}^{(l)}=\\frac{\\exp(e_{ij}^{(l)})}{\\sum_{k\\in \\mathcal{N}(i)}^{}\\exp(e_{ik}^{(l)})}\n",
    "\\tag{3}\\end{equation}$$\n",
    "\n",
    "$$\\begin{equation}\n",
    "h_i^{(l+1)}=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\alpha^{(l)}_{ij} z^{(l)}_j }\\right)\n",
    "\\tag{4}\\end{equation}$$\n",
    "<br>\n",
    "\n",
    "\n",
    "Similar to GCN, update_all API is used to trigger message passing on all the nodes. The message function sends out two tensors: the transformed $z$ embedding of the source node and the unnormalized attention score $e$ on each edge. The reduce function then performs two tasks:\n",
    "\n",
    "*  Normalize the attention scores using softmax (3).\n",
    "*  Aggregate neighbor embeddings weighted by the attention scores (4).\n",
    "\n",
    "Both tasks first fetch data from the mailbox and then manipulate it on the second dimension (dim=1), on which the messages are batched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_func(self, nodes):\n",
    "    # reduce UDF for equation (3) & (4)\n",
    "    # equation (3)\n",
    "    alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "    # equation (4)\n",
    "    h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "    return {'h' : h}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-head Attention\n",
    "\n",
    "GAT introduces multi-head attention to enrich the model capacity and to stabilize the learning process. Each attention head has its own parameters and their outputs can be merged in two ways:\n",
    "\n",
    "$$\\text{concatenation}: h^{(l+1)}_{i} =||_{k=1}^{K}\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)$$\n",
    "\n",
    "$$\\text{average}: h_{i}^{(l+1)}=\\sigma\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{j\\in\\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)$$\n",
    "\n",
    "where $K$ is the number of heads. The authors suggest using concatenation for intermediary layers and average for the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
    "        # multiple head outputs are concatenated together. Also, only\n",
    "        # one attention head in the output layer.\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        return h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the cora dataset\n",
    "Here we load the data set and define the attributes: features, labels, g, mask (training batch)\n",
    "Cora citation network dataset.\n",
    "Nodes mean paper and edges mean citation relationships. Each node has a predefined feature with 1433 dimensions. The dataset is designed for the node classification task. The task is to predict the category of certain paper.\n",
    "To change dataset, change the name in lines 3-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "from dgl.data import CoraGraphDataset\n",
    "dataset = CoraGraphDataset()\n",
    "g = dataset[0]\n",
    "num_class = dataset.num_classes\n",
    "\n",
    "features = g.ndata['feat']\n",
    "\n",
    "mask = g.ndata['train_mask'] \n",
    "\n",
    "labels = g.ndata['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying a different dataset: PubmedGraphDataset\n",
    "Pubmed citation network dataset.\n",
    "Nodes mean scientific publications and edges mean citation relationships. Each node has a predefined feature with 500 dimensions. The dataset is designed for the node classification task. The task is to predict the category of certain publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "from dgl.data import PubmedGraphDataset\n",
    "dataset = PubmedGraphDataset()\n",
    "g = dataset[0]\n",
    "num_class = dataset.num_classes\n",
    "\n",
    "# get node feature\n",
    "features = g.ndata['feat']\n",
    "\n",
    "# get data split\n",
    "mask = g.ndata['train_mask']\n",
    "# val_mask = g.ndata['val_mask']\n",
    "# test_mask = g.ndata['test_mask']\n",
    "\n",
    "# get labels\n",
    "labels = g.ndata['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/kseniiakholina/opt/anaconda3/envs/new_env/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 1.9466 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.9458 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.9449 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.9441 | Time(s) 0.8040\n",
      "Epoch 00004 | Loss 1.9433 | Time(s) 0.7376\n",
      "Epoch 00005 | Loss 1.9425 | Time(s) 0.7363\n",
      "Epoch 00006 | Loss 1.9416 | Time(s) 0.7249\n",
      "Epoch 00007 | Loss 1.9407 | Time(s) 0.7193\n",
      "Epoch 00008 | Loss 1.9398 | Time(s) 0.7189\n",
      "Epoch 00009 | Loss 1.9389 | Time(s) 0.7146\n",
      "Epoch 00010 | Loss 1.9380 | Time(s) 0.7119\n",
      "Epoch 00011 | Loss 1.9370 | Time(s) 0.7075\n",
      "Epoch 00012 | Loss 1.9360 | Time(s) 0.7046\n",
      "Epoch 00013 | Loss 1.9350 | Time(s) 0.7028\n",
      "Epoch 00014 | Loss 1.9340 | Time(s) 0.7058\n",
      "Epoch 00015 | Loss 1.9329 | Time(s) 0.7074\n",
      "Epoch 00016 | Loss 1.9318 | Time(s) 0.7055\n",
      "Epoch 00017 | Loss 1.9306 | Time(s) 0.7028\n",
      "Epoch 00018 | Loss 1.9295 | Time(s) 0.7021\n",
      "Epoch 00019 | Loss 1.9283 | Time(s) 0.6992\n",
      "Epoch 00020 | Loss 1.9270 | Time(s) 0.6967\n",
      "Epoch 00021 | Loss 1.9258 | Time(s) 0.6990\n",
      "Epoch 00022 | Loss 1.9244 | Time(s) 0.6963\n",
      "Epoch 00023 | Loss 1.9231 | Time(s) 0.6940\n",
      "Epoch 00024 | Loss 1.9217 | Time(s) 0.6928\n",
      "Epoch 00025 | Loss 1.9203 | Time(s) 0.6968\n",
      "Epoch 00026 | Loss 1.9188 | Time(s) 0.6982\n",
      "Epoch 00027 | Loss 1.9173 | Time(s) 0.6974\n",
      "Epoch 00028 | Loss 1.9157 | Time(s) 0.6956\n",
      "Epoch 00029 | Loss 1.9142 | Time(s) 0.6941\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# create the model, 2 heads, each head has hidden size 8\n",
    "net = GAT(g,\n",
    "          in_dim=features.size()[1],\n",
    "          hidden_dim=8,\n",
    "          out_dim=7,\n",
    "          num_heads=2)\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "# main loop\n",
    "dur = []\n",
    "for epoch in range(30):\n",
    "    if epoch >= 3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    logits = net(features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[mask], labels[mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >= 3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f}\".format(\n",
    "        epoch, loss.item(), np.mean(dur)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
